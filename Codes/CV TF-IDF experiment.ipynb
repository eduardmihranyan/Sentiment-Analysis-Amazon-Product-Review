{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re,string,unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.metrics import accuracy_score,classification_report,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Musical_instruments_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].fillna(\"\",inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['reviewerID','asin','reviewerName','helpful','unixReviewTime','reviewTime'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review'] = data['reviewText'] + ' ' + data['summary']\n",
    "data.drop(columns = ['reviewText','summary'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing ratings of 1,2,3 with 0 (not good) and 4,5 with 1 (good)\n",
    "data[\"overall\"] = (data[\"overall\"]>3)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            pos = pos_tag([i.strip()])\n",
    "            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n",
    "            final_text.append(word.lower())\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduard/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data.text = data.review.apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data.review,data.overall,test_size = 0.2 , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train = cv.fit_transform(X_train)\n",
    "#transformed test reviews\n",
    "cv_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train=tv.fit_transform(X_train)\n",
    "#transformed test reviews\n",
    "tv_test=tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8334145153433999\n"
     ]
    }
   ],
   "source": [
    "mnb_model=MultinomialNB()\n",
    "\n",
    "mnb_model = mnb_model.fit(cv_train, y_train)\n",
    "\n",
    "mnb_pred = mnb_model.predict(cv_test)\n",
    "\n",
    "score = accuracy_score(y_test, mnb_pred)\n",
    "\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.33      0.31       228\n",
      "           1       0.91      0.90      0.91      1825\n",
      "\n",
      "    accuracy                           0.83      2053\n",
      "   macro avg       0.60      0.61      0.61      2053\n",
      "weighted avg       0.85      0.83      0.84      2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_report = classification_report(y_test, mnb_pred)\n",
    "print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8889430102289333\n"
     ]
    }
   ],
   "source": [
    "mnb_model=MultinomialNB()\n",
    "\n",
    "mnb_model = mnb_model.fit(tv_train, y_train)\n",
    "\n",
    "mnb_pred = mnb_model.predict(tv_test)\n",
    "\n",
    "score = accuracy_score(y_test, mnb_pred)\n",
    "\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.44      0.50      0.47      2053\n",
      "weighted avg       0.79      0.89      0.84      2053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduard/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_report = classification_report(y_test, mnb_pred)\n",
    "print(mnb_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8899171943497322\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2',max_iter=1000,C=1,random_state=0)\n",
    "lr.fit(cv_train, y_train)\n",
    "\n",
    "lr_pred = lr.predict(cv_test)\n",
    "\n",
    "score=accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.94      0.50      0.48      2053\n",
      "weighted avg       0.90      0.89      0.84      2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_bow_report = classification_report(y_test, lr_pred)\n",
    "print(lr_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier( n_estimators=1000,max_depth=4,min_child_weight = 1 ,gamma = 0 ,\n",
    "                    subsample = 0.8, class_weigh =\"balanced\", colsample_bytree = 0.8, nthread=6,seed= 9)\n",
    "\n",
    "xgb.fit(cv_train, y_train)\n",
    "xgb_predict = xgb.predict(cv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889430102289333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, xgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.44      0.50      0.47      2053\n",
      "weighted avg       0.79      0.89      0.84      2053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduard/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "xg_report = classification_report(y_test, xgb_predict)\n",
    "print(xg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier( n_estimators=1000,max_depth=4,min_child_weight = 1 ,gamma = 0 ,\n",
    "                    subsample = 0.8, class_weigh =\"balanced\", colsample_bytree = 0.8, nthread=6,seed= 9)\n",
    "\n",
    "xgb.fit(tv_train, y_train)\n",
    "xgb_predict = xgb.predict(tv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889430102289333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, xgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.44      0.50      0.47      2053\n",
      "weighted avg       0.79      0.89      0.84      2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xg_report = classification_report(y_test, xgb_predict)\n",
    "print(xg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=7,C=1,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight='balanced', random_state=7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(cv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_predict = svc.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899171943497322"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, svc_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.94      0.50      0.48      2053\n",
      "weighted avg       0.90      0.89      0.84      2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_report = classification_report(y_test, svc_predict)\n",
    "print(svc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=7,C=1,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight='balanced', random_state=7)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(tv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_predict = svc.predict(tv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8904042864101315"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, svc_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.03       228\n",
      "           1       0.89      1.00      0.94      1825\n",
      "\n",
      "    accuracy                           0.89      2053\n",
      "   macro avg       0.95      0.51      0.48      2053\n",
      "weighted avg       0.90      0.89      0.84      2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_report = classification_report(y_test, svc_predict)\n",
    "print(svc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
